1. 11.19 due, connect sample db with frontend done (ami done if neccessary)
2. 11.23 due, ami and asg done
3. 11.26 due, whole done. (test)

plan a: 
memory table:  userid(key), number of tweets (less than 1G)
sql: userid(key), retweet list
hbase: time(key), tweetid:tweet_text List

plan b: 
sql userid(key), number of tweets, retweet list (less than 10G)
hbase: time(key), tweetid:tweet_text List

plan c:
hbase: (userid(index), time(index)) two key,  number of tweets, retweet list, tweetid:tweet_text List

so the csv we need:
1. (userid(index), time(index)) two key,  number of tweets, retweet list, tweetid:tweet_text List

split to time(key), tweetid:tweet_text List   and userid(key), number of tweets, retweet list 

than split to all in plan a

@Channing at Nov 12, 2013
Progress: converting s3 file into csv file for memory table. It's believable that the memory table can be less than 800MB,
and the combination of memroy table and sql table (schema: <userId, number of tweets, list of retweet userIds>) can be less
than 1.5GB.
Issue: convert s3 file into table costs a lot of memory (seems impossible in one regular machine) and time (2.5 days
in my laptop). 
How to fix: 1. launch aws instances; 2. pass tag check?


11.12 meeting:

csv we need
 userid, time, tweetid:tweet, retweet target   (14 files)
 
QIAN: 
	userid, time, tweetid:tweet   (read and write line by line, no space need)  --> to hbase

YINSU:
	userid, number of tweets, retweet list (to 14 files and sorted by userid. take care of virtual space)
	
	
11.14 meeting:

QIAN:
	find user max, user min, load data into HBASE
YINSU:
	external merge and get:
	userid, number of tweets
	userid, retweet list   ---> introduce to mysql
ZHUOLIN:
	get userid, number of tweets  csv, try to load file when service start (tomcat)