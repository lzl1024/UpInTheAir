1. 11.19 due, connect sample db with frontend done (ami done if neccessary)
2. 11.23 due, ami and asg done
3. 11.26 due, whole done. (test)

plan a: 
memory table:  userid(key), number of tweets (less than 1G)
sql: userid(key), retweet list
hbase: time(key), tweetid:tweet_text List

plan b: 
sql userid(key), number of tweets, retweet list (less than 10G)
hbase: time(key), tweetid:tweet_text List

plan c:
hbase: (userid(index), time(index)) two key,  number of tweets, retweet list, tweetid:tweet_text List

so the csv we need:
1. (userid(index), time(index)) two key,  number of tweets, retweet list, tweetid:tweet_text List

split to time(key), tweetid:tweet_text List   and userid(key), number of tweets, retweet list 

than split to all in plan a

@Channing at Nov 12, 2013
Progress: converting s3 file into csv file for memory table. It's believable that the memory table can be less than 800MB,
and the combination of memroy table and sql table (schema: <userId, number of tweets, list of retweet userIds>) can be less
than 1.5GB.
Issue: convert s3 file into table costs a lot of memory (seems impossible in one regular machine) and time (2.5 days
in my laptop). 
How to fix: 1. launch aws instances; 2. pass tag check?


11.12 meeting:

csv we need
 userid, time, tweetid:tweet, retweet target   (14 files)
 
QIAN: 
	userid, time, tweetid:tweet   (read and write line by line, no space need)  --> to hbase

YINSU:
	userid, number of tweets, retweet list (to 14 files and sorted by userid. take care of virtual space)
	
	
11.14 meeting:

##QIAN:
##	find user max, user min, load data into HBASE

YINSU:
	record: long userid, string time, long tweetid, string tweet, long retweet target 
	two comparator (userid, retweet target)

	sort by userid
	internal merge file by userid, external merge file, wape out same tweetid
	method: 
		internal merge : collection sort, 
		external merge: priority queue with file descripter : reference ReducerPerform.txt
	
	make a function: out file 1 : userid, number of tweets --- output to a file  -- hand to ZHUOLIN
	out file 2 : userid, time, tweetid:tweet, retweet target   (19 files) -- hand to QIAN

	use file 2 to sort by retweet target
	internal merge and external (19 files)
	make a function : out file 3 : userid, retweet list --- output to one file
QIAN:
	rerun 0,13,14:
	time, tweetid:tweet	 --> unique tweetid
	userid, retweet list -----> introduce to mysql
ZHUOLIN:
	write program for YINSU
	get userid, number of tweets  csv, try to load file when service start (tomcat)
	
11.15:
	Assign to Yinsu Chu:
	
	Intruction of MySQL Import & Export:
	1. Create a table:
		mysql> create table test( userId bigint, time varchar(100), tweet blob, retweet_userId bigint, primary key (userId, time) );                                          
	2. Import 19 result_split_fileList_X.csv into the table:
		mysql> load data infile '/Users/maoqian/cloudProject/split_file_list/result_split_fileList_X.csv' replace into table test FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '"' ESCAPED BY '"'
	3. Export to csv file using certain fields (please change field based on need):
		mysql> select time, tweet from test into outfile '/Users/maoqian/cloudProject/text_exported_table3.csv' FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '"' ESCAPED BY '"';
	4. Finally, create a new table based on need. Import the csv file exported in Step 3.
	
